<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html lang="ja-JP">
<head>
<title>Face and Gaze Measurement System</title>
<meta http-equiv="Content-Type" content="text/html; charset=iso-2022-jp">
<meta name="keywords" content="Vision, Robot, System">
<meta http-equiv="Content-Style-Type" content="text/css">
</head>

<body text="#333333" background="background.gif" bgcolor="#FFFFFF" link="#00008B" vlink="#696969" alink="#FF0000">

<div align="center">
 <table width="100%" border="0" cellspacing="0" cellpadding="2">
  <tr><td width="100%" bgcolor="#bbbbbb" align="center"> </td></tr>
  <tr>
   <td width="100%" bgcolor="#bbbbbb" align="center">
    <font size=5 color="#222222">ヒューマンモデリングおよびヒューマンインタフェースのための<br>顔情報計測システム</font>
   </td>
  </tr>
  <tr><td width="100%" bgcolor="#bbbbbb" align="center"></td></tr>
  <tr>
   <td width="100%" align="center">
    <font size=2>
     奈良先端科学技術大学院大学 ロボティクス講座<br>
     作成 2003年11月24日<br>
     更新 2003年11月24日
    </font>
   </td> 
  </tr>
 </table>
</div>

<div align="center">
 <table width="100%" border="0" cellspacing="2" cellpadding="2">
  <tr>
   <td width="100%" bgcolor="#bbbbbb" align="center">
    <font color="#222222">1. はじめに</font>
   </td>
  </tr>
 </table>
</div>

<div align="left">
本稿では人の顔に関する様々な情報をリアルタイムに計測する技術の概要およびその応用について述べる．開発したシステムは，顔の動き，視線方向，瞬目などを非接触で計測できるものである．その実装は，汎用で安価なハードウェアを用いたシステムにより実現されている．また，開発したシステムを用いて構築した，ヒューマンモデリングおよびヒューマンインタフェースに関する様々な応用システムについても紹介する．
</div>

<br>

<div align="center">
 <table width="100%" border="0" cellspacing="2" cellpadding="2">
  <tr>
   <td width="100%" bgcolor="#bbbbbb" align="center">
    <font color="#222222">2. 開発の背景</font>
   </td>
  </tr>
 </table>
</div>

<div align="left">
行動をモデリングする，あるいは知的なインタフェースを構築するためには，人間の行動計測が欠かせない．特に人間の顔は，注意や意図，表情といった心理状態に関連する様々な変化を表出するため，その変化を定量的に計測することは重要である．しかし，そのような技術は現状ではほとんど実用化されていない．
<br>
まず視線に関しては，アイカメラと呼ばれる装置を頭部に装着して計測するのが一般的である．しかし，頭部への固定や視野の遮蔽による被験者への負担が大きく，また装着ずれは精度低下につながるため，長時間の実験は難しい．また頭部姿勢を計測するには別途，磁気センサが必要で，さらに瞬目を計測するには顔表面に電極を取り付けることが一般的である．これでは，被験者の自然な状態での計測を行うことは難しい．このような計測装置に関する制約から，従来のヒューマンモデリングに関する研究ではプラントオペレータやテストドライバーのように限定された人の顔情報（主に視線情報）を計測することしかできなかった．
<br>
しかし，少子・高齢化社会，バリアフリー社会の到来にともない，より人にやさしいコンピュータや機械，ロボットのなどのシステムを構築する必要性が高まっており，その実現へ向けて今後は一般の人（乳幼児，高齢者なども含む）の計測が不可欠である．そのためには非接触かつ自然な状態で顔情報を計測できるシステムが必要となる．
</div>

<br>

<div align="center">
 <table width="100%" border="0" cellspacing="2" cellpadding="2">
  <tr>
   <td width="100%" bgcolor="#bbbbbb" align="center">
    <font color="#222222">3. 計測手法</font>
   </td>
  </tr>
 </table>
</div>

<div align="center">
 <table border=1>
 <tbody>
  <tr>
   <td width="500" align="center">
    <div align="center">
     <img src="https://robotics.naist.jp/~kento-ta/images/1_default_face.gif" alt=""><br>
     <font size="1">図１：顔発見用テンプレート</font>
    </div>
   </td>
   <td>
3.1 顔発見モジュール<br>
<br>
これは画像中から顔を見つけて個人識別を行うことで，計測対象の人物を特定するためのモジュールである．これは顔計測モジュールの初期化（3次元顔モデルの選択）に当たる．顔の発見は，画像全体に対し，図１に示すようなテンプレート画像を用いたマッチング処理により行う．このマッチングには，明るさの変化に影響されにくいように，エッジ画像を用いる．また，顔全体を包含する低解像度のテンプレート画像を用いており，1枚の画像を用いて不特定のユーザの顔を発見することが可能である．このステレオ視の結果，カメラから一定距離内に任意のユーザがいる時に，顔領域を切り出すことが可能になった．またオンラインでの顔識別には，商用のソフトウェアであるTrueFace（eTrue社）を用いた．<br>
   </td>
  </tr>
  <tr>
   <td width="500" align="center">
    <div align="center">
     <img src="https://robotics.naist.jp/~kento-ta/images/1_face_model.gif" alt=""><br>
     <font size="1">図２：顔トラッキング用テンプレート<br></font>
     <br>
     <img src="https://robotics.naist.jp/~kento-ta/images/2_track_result.gif" alt=""><br>
     <font size="1">図３：顔トラッキングの結果<br></font>
     <br>
     <img src="https://robotics.naist.jp/~kento-ta/images/3_gaze_model.gif" alt=""><br>
     <font size="1">図４：視線方向推定手法<br></font>
     <br>
     <img src="https://robotics.naist.jp/~kento-ta/images/4_tracking_graph1.gif" alt="">
     <img src="https://robotics.naist.jp/~kento-ta/images/4_geoface2.gif" alt=""><br>
     <font size="1">図５：頭部・視線計測の結果表示<br></font>
    </div>
   </td>
   <td>
3.2 顔計測モジュール<br>
<br>
これは，顔の位置，姿勢，視線方向，瞬目，口の開閉などを実時間で計測するモジュールである．これらのデータは，顔に関する最も基本的な物理量である．まず顔の位置・姿勢をトラッキングするには，図２に示すような顔発見用テンプレート画像と，それらの三次元座標を利用する．ここではまず，顔トラッキング処理について述べる．3次元顔モデルと3次元観測値の信頼性に基づく重み付けを考慮したモデルフィッティング処理により，高速に頭部の位置・姿勢6自由度を求めることができる[1]．<br>
<div align="center">
 <img src="https://robotics.naist.jp/~kento-ta/images/equation_trans.gif" alt="">
</div>
ここで，Nは特徴の数，xiは顔モデル中のある特徴の位置ベクトル，yiは対応する特徴が観測された位置ベクトル，wiは観測の信頼性，T, Rは並進ベクトルおよび回転行列，そしてEはフィッティングエラーである．最適なフィッティングはE を最小化するT, Rを求める問題に帰着でき，本ソフトウェアではこれを最急降下法により解いている．テンプレートマッチングに正規化相関を，また信頼性wiにその相関値を用いることで，図３に示すように顔の回転や隠蔽，照明変動に強い顔トラッキングが実現できた．次に，目の虹彩の位置から視線方向を推定する手法について述べる．顔トラッキングにより顔の位置・姿勢が計測できると，（頭部に固定された）眼球の中心位置を推定することができる．画像から得られる虹彩の中心位置と推定された眼球中心位置の関係より，水平・垂直の視線角度がわかるので，三次元ベクトルとして視線が得られる．この手順を図４に示す．また，眉，口の変形は，頭部位置・姿勢推定の後処理として別に特徴点追跡を行うことで，瞼（まぶた）はエッジ検出を行うことで，変形量を計測することができるようになった．このように計測したパラメータは，図５（左）に示すように顔画像の上に表示したり，図５（右）のようにCG画像として表現することができる．<br>
   </td>
  </tr>
  <tr>
   <td width="500" align="center">
    <div align="center">
     <img src="https://robotics.naist.jp/~kento-ta/images/gesture_image.gif" alt=""> <br>
     <font size="1">図６：ジェスチャの認識</font>
    </div>
   </td>
   <td>
3.3 顔行動抽出モジュール<br>
<br>
最後は計測した物理量に基づき，注視対象，ジェスチャ，表情などを実時間で推定モジュールである．これらの情報は，計測された物理量の組み合わせから抽出されるシンボル情報である．ジェスチャ認識では，頭部の６自由度の動きパラメータと連続DPを利用し，"Yes", "No", "Look Left", "Zoom In"等のジェスチャを認識できるようにした．次に，表情認識では眉，口など特徴点の位置関係を用いて，"口を開けている"，"片目を閉じている"のような動きを認識できるようにした．これらの認識の様子を図６に示す．また，環境中の対象物の位置と大きさを定義できれば，頭部位置から出した視線ベクトルが，対象を通過するかどうかを各対象について判定することで，容易に注視対象物を特定できる．<br>
   </td>
  </tr>
 </table>
</div>

<br>

<div align="center">
 <table width="100%" border="0" cellspacing="2" cellpadding="2">
  <tr>
   <td width="100%" bgcolor="#bbbbbb" align="center">
    <font color="#222222">4. システムの実装</font>
   </td>
  </tr>
 </table>
</div>

<div align="left">
前章で述べたモジュールを実装するにあたっては，リアルタイム処理の実現を重視した．そのために，もっとも処理の重い相関演算によるテンプレートマッチングの処理の一部に，Intel Pentium系プロセッサのMMX命令を利用した．この高速化により，Pentium4 2.53GHz を用いた場合，顔発見は約70ms，顔情報計測は約10msで処理を実現できた．開発した顔情報計測システムは，顔の位置・姿勢に関してはそれぞれ約2mm, 2度の精度で，また視線については約5度の精度で計測することができ，また以下の特徴を持つ．
<ul style="margin-top:2pt; margin-bottom:2pt;">
 <li>完全に非接触，非侵襲，受動的．
 <li>顔が比較的大きく動いても計測が可能．
 <li>視線と同時に顔の動きも計測できる．
 <li>自動で顔モデルを構築することが可能．
 <li>瞬き，口の開閉など顔の特徴の変化や，ジェスチャも計測可能．
 <li>特別なハードウェアを用いず，汎用PCだけ処理を実現しているので安価．
 <li>用途（速度，精度，サイズなど）に応じて様々なPCおよびカメラを選択できる．
 <li>共有メモリおよびソケット通信によるデータ出力ができ，アプリケーションの構築が容易．
</ul>
なお，本ソフトウェアを実装するハードウェア構成として，カメラについては，
<ul style="margin-top:2pt; margin-bottom:2pt;">
 <li>IEEE1394高速度ステレオカメラ
 <li>IEEE1394高画質カメラ×2台
 <li>NTSCカメラ×2台
</ul>
などから，PCについては
<ul style="margin-top:2pt; margin-bottom:2pt;">
 <li>高性能デスクトップパソコン
 <li>小型ノートパソコン
</ul>
から，処理スピード，価格，実験環境などに応じて選択することができる．最も速いシステムでは60Hzでの処理を，また最も安価なシステムでは10万円程度でのシステム構築を，またポータブルなシステムでは1.5kg程度でのシステム構築を可能とした．<br>
<div align="center">
 <img src="https://robotics.naist.jp/~kento-ta/images/5_system_desktop_1394.jpg" alt="">
 <img src="https://robotics.naist.jp/~kento-ta/images/5_system_desktop_MEGAD.jpg" alt="">
 <img src="https://robotics.naist.jp/~kento-ta/images/5_system_note_NTSC.jpg" alt=""><br>
 <font size="1"> 図７：様々なシステム構成</font>
</div>
</div>

<br>

<div align="center">
 <table width="100%" border="0" cellspacing="2" cellpadding="2">
  <tr>
   <td width="100%" bgcolor="#bbbbbb" align="center">
    <font color="#222222">5. 応用システムの構築</font>
   </td>
  </tr>
 </table>
</div>

<div align="center">
 <table border=1>
 <tbody>
  <tr>
   <td colspan="2" align="center">
   5.1 ヒューマンモデリング
   </td>
  </tr>
  
  <tr>
   <td width="500" align="center">
    <div align="center">
     <img src="https://robotics.naist.jp/~kento-ta/images/6_hiraki_exp.jpg" alt=""> <br>
     <img src="https://robotics.naist.jp/~kento-ta/images/6_baby1_crop.gif" alt="">
     <img src="https://robotics.naist.jp/~kento-ta/images/6_baby2_crop.gif" alt="">
     <img src="https://robotics.naist.jp/~kento-ta/images/6_baby3_crop.gif" alt=""> <br>
     <font size="1">図８：乳幼児の心理実験の計測結果</font>
    </div>
   </td>
   <td>
5.1.1 認知心理学のための計測システム<br>
<br>
従来の認知心理学の実験では，視線計測には頭部装着型のシステムを用い，また顔の動きを記録するにはビデオ録画した後に手動で解析する手法が取られていた．特に乳幼児の実験には装着型の視線計測装置を用いることができなく，非接触型の顔・視線計測システムの実現が望まれていた．図８は，開発したシステムを乳幼児の計測に用いた様子である．これは被験者の乳幼児（この場合は生後12ヶ月）がテレビ画面に視覚刺激が写っているのを見ている注視時間から，どの程度映像を理解できているかを調る実験である[2]．乳幼児の顔は左右に動くが，そのような場面でもほとんどの部分で計測が可能であった．また，大きな動きにより計測が失敗した場面でも，顔が前を向いた瞬間に再び計測が始められた．<br>
   </td>
  </tr>
  <tr>
   <td width="500" align="center">
    <img src="https://robotics.naist.jp/~kento-ta/images/7_fixation_map.gif" alt=""> <br>
    図９：ドライバの計測
   </td>
   <td>
5.1.2 ドライバ行動計測システム<br>
<br>
自動車のドライバの挙動を計測・記録できるシステムを構築した[3]．センサとしては，カメラの他にGPS，加速度センサ等が搭載されており，顔情報を含めた全ての計測データは同時にファイルに記録される．車内の対象物への注視時間を計測することによって，ドライバの行動推定を行ったり，道路領域の画像を撮影し，その上に視線方向や頭部方向を投影し，記録するためのシステムを構築した．車外画像の撮影には，全方位視覚センサを用いることで，広い視野の画像を確保した．また，夜間でも赤外線投光器を利用することにより計測できる．これら全ての情報はオンラインでも利用できるので，これらの情報をもとにドライバの運転支援を行うことも可能である．<br>
   </td>
  </tr>
  <tr>
   <td colspan="2" align="center">
   5.2 ヒューマンインタフェース
   </td>
  </tr>
  <tr>
   <td width="500" align="center">
    <img src="https://robotics.naist.jp/~kento-ta/images/8_handsfreemouse.gif" alt=""><br>
    図１０：ハンズフリーマウス
   </td>
   <td>
5.2.1 ハンズフリーマウス<br>
<br>
顔の動きによりポインタを操作するコンピュータインタフェースを構築した．ジョイスティックのように顔の向きを使って上下左右にマウスカーソルを動かすことができる．また，左右の目のウィンクで，左クリック，右クリックの動作をエミュレートした．現状では，マウスなど他のポインティングデバイスと比較して使い勝手がよいとは言えないが，手を使えない状況でも非接触でコンピュータを操作することが可能であるため，福祉分野などへの展開が期待できる．<br>
   </td>
  </tr>
  <tr>
   <td width="500" align="center">
    <img src="https://robotics.naist.jp/~kento-ta/images/9_aska_bright.gif" alt=""><br>
    図１１：ロボットインタラクション
   </td>
   <td>
5.2.2 ロボット対話インタフェース<br>
<br>
我々はこれまでに，人間型ロボットを用いた対話システムの研究を行ってきた．このシステムは，音声認識によりユーザの発話を理解しジェスチャを交えながら質問に答えるのであるが，この対話システムに顔情報計測ソフトウェアを組み込んだ．顔情報計測を用いると，ユーザの顔や視線，口の開閉を認識することができる．これにより，視線や顔が自分に向いていることを確認した時のみ音声入力を受け付け，背景雑音には反応しないようなシステムを実現した[4]．ロボットがユーザの視線を意識しながらコミュニケーションを行うという意味で，ロボットと人間のある種の「アイコンタクト」が実現できたと言える．<br>
   </td>
  </tr>
  <tr>
   <td width="500" align="center">
    <img src="https://robotics.naist.jp/~kento-ta/images/10_watson_new.jpg" alt=""> <br>
    図１２：視線操縦型車いす
   </td>
   <td>
5.2.3 視線操縦型車いす<br>
<br>
人間は通常，行きたい方向を見る．このことを利用して，視線や顔の方向に基づいて走行を行う車いすシステムを開発した[5]．このシステムは屋内用の電動車いすM-Smart（ミサワホーム）をベースに作られている．顔計測用カメラに加え，搭乗者の足もとにはレーザ距離センサが搭載され，ロボット前方の距離情報をスキャンする．車いすは，基本的にはユーザが見ている方向に進むが，障害物回避機能と組み合わせることにより壁のポスターを見ていてもそこに衝突することはなく，停止したり，壁に沿って滑らかに移動することができる．また発進や停止を指示するには“うなずき”，“首振り”のジェスチャを行う．この車いすシステムは，屋内・屋外を含めて様々な場所においてデモを行っており，顔情報計測の安定性の高さを示している．<br>
   </td>
  </tr>
 </table>
</div>

<!--
<div align="center">
 <table width="100%" border="0" cellspacing="2" cellpadding="2">
  <tr>
   <td width="100%" bgcolor="#bbbbbb" align="center">
    <font color="#222222">実験ビデオ</font>
   </td>
  </tr>
 </table>
</div>

<div align="center">
 <table border=2>
 <tbody>
  <tr>
   <td><a href="indoor.mpg">屋内走行実験</a><br>(MPEG1, 7.8MB)</td>
   <td><a href="outdoor.mpg">屋外走行実験</a><br>(MPEG1, 1.9MB)</td>
   <td><a href="robofesta.mpg">ロボフェスタ関西・プレ大会</a><br>(MPEG1, 8.3MB)</td>
  </tr>
  <tr>
   <td><a href="../../MOVIE/wchair/indoor.mpg"><img src="indoor.jpg" alt="indoor"></a></td>
   <td><a href="../../MOVIE/wchair/outdoor.mpg"><img src="outdoor.jpg" alt="outdoor"></a></td>
   <td><a href="../../MOVIE/wchair/robofesta.mpg"><img src="robofesta.jpg" alt="robofesta"></a></td>
  </tr>
 </table>
</div>
-->

<br>

<div align="center">
 <table width="100%" border="0" cellspacing="2" cellpadding="2">
  <tr>
   <td width="100%" bgcolor="#bbbbbb" align="center">
    <font color="#222222">６．おわりに</font>
   </td>
  </tr>
 </table>
</div>

<div align="left">
本稿で述べた顔計測技術は従来の視線計測技術を補完する実用的なものである．今後はソフトウェアの使い勝手をさらに向上させ，実用化を目指すと共に，完成度の高い応用システムを開発していく予定である．<br>
なお，本研究・開発の一部は，情報処理振興事業協会・未踏ソフトウェア創造事業（平成14年度，テーマ名：ヒューマンモデリングのための顔情報計測ソフトウェアの開発）として行われた．
</div>

<br>

<div align="center">
 <table width="100%" border="0" cellspacing="2" cellpadding="2">
  <tr>
   <td width="100%" bgcolor="#bbbbbb" align="center">
    <font color="#222222">文献リスト</font>
   </td>
  </tr>
 </table>
</div>

<div align="left">
 <ol style="margin-top:2pt; margin-bottom:2pt;">
  <li>Y.Matsumoto and A.Zelinsky: "An Algorithm for Real-time Stereo Vision Implementation of Head Pose and Gaze Direction Measurement," Proc of IEEE Fourth Int. Conf. on Face and Gesture Recognition, pp.499-505, 2000.
  <li>有田 亜希子, 開 一夫, 小松 孝徳, 松本 吉央:“人間と機械の共同注意に関する研究”, 日本認知科学会第18回大会予稿集, pp.204-205, 2001.
  <li>K.Takemura, J.Ido, Y.Matsumoto and T.Ogasawara: "Drive Monitoring System Based on Non-Contact Measurement System of Driver's Focus of Visual Attention," Proc of IEEE Intelligent Vehicles Symposium, 2003.
  <li>J.Ido, Y.Myouga, Y.Matsumoto and T.Ogasawara: "Interaction of Receptionist ASKA Using Vision and Speech Information," Proc of Int. Conf. on Multisensor Fusion and Integration for Intelligent Systems, 2003.
  <li>Y.Matsumoto, T.Ino and T.Ogasawara: "Development of Intelligent Wheelchair System with Face and Gaze Based Interface," Proc. of 10th IEEE Int. Workshop on Robot and Human Communication, 262-267, 2001.
 </ol>
</div>

<br>

<div align="center">
 <table width="100%" border="0" cellspacing="2" cellpadding="2">
  <tr>
   <td width="100%" bgcolor="#bbbbbb" align="center">
    <font color="#222222">メディアでの紹介</font>
   </td>
  </tr>
 </table>
</div>

<div align="left">
 <ul style="margin-top:2pt; margin-bottom:2pt;">
  <li>1998.08.18 The Australian（オーストラリアの全国紙）<br>
       "Facing up to the future"
  <li>2002.10.19 朝日新聞夕刊<br>
       Science &amp; Technology−ハイテク顔パス「コンビニ・福祉 広がる応用分野」
  <li>2003.04.24 日本テレビ スームイン!!SUPER<br>
       「今日のイチオシ**ナマやねん!!**」
 </ul>
</div>

<!--
<div align="center">
 <table border=2>
 <tbody>
  <tr>
   <td><a href="takuro.mpg">辰巳さんも試乗</a><br>(MPEG1, 32秒, 2.3MB)</td>
   <td><a href="news.mpg">ニュース</a><br>(MPEG1, 50秒, 3.7MB)</td>
  </tr>
  <tr>
   <td><a href="takuro.mpg"><img src="wchair1.jpg" alt="movie1"></a></td>
   <td><a href="news.mpg"><img src="wchair2.jpg" alt="movie2"></a></td>
  </tr>
 </table> 
</div>
-->

<br>

<div align="center">
 <table width="100%" border="0" cellspacing="2" cellpadding="2">
  <tr>
   <td width="100%" bgcolor="#bbbbbb" align="center">
    <font color="#222222">デモの記録</font>
   </td>
  </tr>
 </table>
</div>

<div align="left">
 <ul style="margin-top:2pt; margin-bottom:2pt;">
  <li>国際ロボット展（2003年11月，東京ビッグサイト）
  <center>
   <img src="https://robotics.naist.jp/~kento-ta/images/RobotTen1.jpg" alt="kokusai robot ten">
   <img src="https://robotics.naist.jp/~kento-ta/images/RobotTen2.jpg" alt="kokusai robot ten">
  </center>
 </ul>
</div>

<!--
<div align="center">
 <table width="100%" border="0" cellspacing="2" cellpadding="2">
  <tr>
   <td width="100%" bgcolor="#bbbbbb" align="center">
    <font COLOR="#222222">関連研究</font>
   </td>
  </tr>
 </table>
</div>

<div align="left">
 <ul style="margin-top:2pt; margin-bottom:2pt;">
  <li>大阪大学・白井研究室 <a href="http://www-cv.mech.eng.osaka-u.ac.jp/research/hi-j.html">顔の視覚情報処理を用いた知的車椅子に関する研究</a>
 </ul>
</div>
-->

<br>

<div align="center">
 <table width="100%" border="0" cellspacing="2" cellpadding="2">
  <tr>
   <td width="100%" bgcolor="#bbbbbb" align="center">
    <font color="#222222">研究メンバー</font>
   </td>
  </tr>
 </table>
</div>

<div align="left">
 現在のメンバー：
 <ul style="margin-top:2pt; margin-bottom:2pt;">
  <li>松本吉央
  <li>怡土順一
  <li>小枝正直
  <li>竹村憲太郎
 </ul>
 
 過去のメンバー：
 <ul style="margin-top:2pt; margin-bottom:2pt;">
  <li>後藤健志（2001年度修了）：相関演算のMMX化
  <li>紙　弘和（2000年度修了）：CardBusキャプチャカードのデバイスドライバ開発
 </ul>
</div>

<br>

<div align="left">
 <table width="100%" border="0" cellspacing="2" cellpadding="2">
  <tr>
   <td width="100%" bgcolor="#bbbbbb" align="center">
    <font color="#222222">連絡先</font>
   </td>
  </tr>
 </table>
</div>

<div align="left">
松本吉央 (<img src="https://robotics.naist.jp/~kento-ta/images/mail_address.gif" alt="mail address">)<br>
<br>
<a href="http://robotics.naist.jp">研究室トップへ戻る</a>
</div>

<div align="right">
 <a href="http://validator.w3.org/check/referer"><img border="0" src="valid-html401.gif" alt="Valid HTML 4.01!" height="31" width="88"></a>
</div>

<script src="http://www.google-analytics.com/urchin.js" type="text/javascript">
</script>
<script type="text/javascript">
_uacct = "UA-414006-1";
urchinTracker();
</script>
</body>
</html>
